{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:2920: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000-0000-000b-012c-135efde15401\n",
      "[1.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import skew\n",
    "import numpy as np\n",
    "import csv\n",
    "import datetime\n",
    "from joblib import load\n",
    "\n",
    "def feature_extraction(dist_con_weekly, interval_ind):\n",
    "    kt = kurtosis(dist_con_weekly)\n",
    "    sk = skew(dist_con_weekly)\n",
    "    peak = max(dist_con_weekly)\n",
    "    start = 0\n",
    "    rec_len = 0\n",
    "    rec_temp = []\n",
    "    final_rec_len = []\n",
    "    final_rec_number = []\n",
    "    final_rec_avg = []\n",
    "    \n",
    "    #different unsupervised parameters for 15/30 intervals\n",
    "    if interval_ind == 30:\n",
    "        int_diff = 0.8\n",
    "        diff_perc = 0.2\n",
    "        len_al = 2\n",
    "    else:\n",
    "        int_diff = 0.4\n",
    "        diff_perc = 0.2\n",
    "        len_al = 3\n",
    "    \n",
    "    for i in range(1, len(dist_con_weekly)):\n",
    "        #finding high sustained energy usage and record durations and average consumptions+\n",
    "        if dist_con_weekly[i] - dist_con_weekly[i-1] > int_diff and start == 0:\n",
    "            rec_temp = [dist_con_weekly[i]]\n",
    "            start = i\n",
    "        elif start != 0 and abs(dist_con_weekly[i]/dist_con_weekly[start] - 1) <= diff_perc:\n",
    "            rec_temp.append(dist_con_weekly[i])\n",
    "            rec_len += 1\n",
    "        #elif dist_con_weekly[i] > 1 and start != 0 and (dist_con_weekly[i]/dist_con_weekly[start] - 1) > 0.1:\n",
    "        #    rec_temp.append(dist_con_weekly[i])\n",
    "        #    rec_len += 1\n",
    "        else:\n",
    "            if rec_len >= len_al:\n",
    "                final_rec_len.append(rec_len)\n",
    "                final_rec_number.append(rec_temp)\n",
    "                final_rec_avg.append(np.mean(rec_temp))\n",
    "                start = 0\n",
    "                rec_len = 0\n",
    "                rec_temp = []\n",
    "            else:\n",
    "                start = 0\n",
    "                rec_len = 0\n",
    "                rec_temp = []\n",
    "\n",
    "    #print(np.mean(final_rec_len))\n",
    "    return [sk,kt,peak,np.mean(final_rec_len),len(final_rec_len),np.mean(final_rec_avg)]\n",
    "\n",
    "def interval(item):\n",
    "    with open(item, 'r') as fin:\n",
    "        r = csv.reader(fin)\n",
    "        next(r)\n",
    "        row_next = next(r)\n",
    "        dt = row_next[4].split('T')[0] + ' ' + row_next[4].split('T')[1][:-5]\n",
    "        temp_dt_1 = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S').minute\n",
    "        row_next = next(r)\n",
    "        dt = row_next[4].split('T')[0] + ' ' + row_next[4].split('T')[1][:-5]\n",
    "        temp_dt_2 = datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S').minute\n",
    "        #print(temp_dt_1, temp_dt_2)\n",
    "        if abs(temp_dt_2 - temp_dt_1) == 15:\n",
    "            interval_ind = 15\n",
    "        elif abs(temp_dt_2 - temp_dt_1) == 30:\n",
    "            interval_ind = 30\n",
    "        else:\n",
    "            interval_ind = 0         \n",
    "        return interval_ind\n",
    "\n",
    "def compressing_file_unsupervised(item, interval_ind):\n",
    "    with open(item, 'r') as fin:\n",
    "        nan_cnt = 0\n",
    "        ev_sig_cnt = 0\n",
    "        features = []\n",
    "        r = csv.reader(fin)\n",
    "        next(r)\n",
    "        dist_con_weekly = []\n",
    "        temp_timestamp = []\n",
    "        history = []\n",
    "        weekly_features = []\n",
    "        file_id = []\n",
    "        for row in r:\n",
    "            #make sure it's the same house\n",
    "            if row[0] not in file_id:\n",
    "                file_id.append(row[0])\n",
    "            #reorganize timestamp making it python readable\n",
    "            dt = row[4].split('T')[0] + ' ' + row[4].split('T')[1][:-5]\n",
    "            #print(dt)\n",
    "            #trying to see the length of the data\n",
    "            history_factor = 2019 - datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S').year\n",
    "            if history_factor == 0:\n",
    "                history_factor = 0.5\n",
    "            history.append(history_factor)\n",
    "            if 3 <= datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S').month <= 5 or 9 <= datetime.datetime.strptime(dt, '%Y-%m-%d %H:%M:%S').month <= 11:\n",
    "                wd = datetime.date(int(row[7]), int(row[8]), int(row[9])).weekday()\n",
    "                #print(wd)\n",
    "                #discard weekends...\n",
    "                if wd == 6 or wd == 0:\n",
    "                    if dist_con_weekly != []:\n",
    "                        line = feature_extraction(dist_con_weekly, interval_ind)\n",
    "                        if np.isnan(line[2]) == True or np.isnan(line[3]) == True:\n",
    "                            nan_cnt += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            ev_sig_cnt += 1\n",
    "                            weekly_features.append(line)\n",
    "                        dist_con_weekly = []\n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    con = float(row[1])\n",
    "                    #treating duplicate values...\n",
    "                    if dt not in temp_timestamp:\n",
    "                        temp_timestamp.append(dt)\n",
    "                        dist_con_weekly.append(con)\n",
    "                    else:\n",
    "                        continue\n",
    "            else:\n",
    "                continue\n",
    "    #calculate final compressed features mean\n",
    "    output_feature = np.mean(weekly_features, axis=0)\n",
    "    \n",
    "    #showing selection scores and null/score ratios, important to implement cutoff rate\n",
    "    if interval_ind == 15:\n",
    "        ratio_threshold = 0.004\n",
    "    elif interval_ind == 30:\n",
    "        ratio_threshold = 0.008\n",
    "    else:\n",
    "        ratio_threshold = 0\n",
    "    #calculate unsupervised edge scores...\n",
    "    if nan_cnt > 0:\n",
    "        freq_ratio = float(ev_sig_cnt/nan_cnt)\n",
    "        factor = float(freq_ratio/history[0])\n",
    "        if factor >= ratio_threshold:\n",
    "            f = factor\n",
    "        else:\n",
    "            f = 0\n",
    "    else:\n",
    "        f = 0\n",
    "    #final sanity check on multipe file id in one file...\n",
    "    if len(file_id) == 1:\n",
    "        f_id = file_id[0]\n",
    "        return f_id, output_feature, f\n",
    "    else:\n",
    "        return 'none', output_feature, 0\n",
    "    #print('finished')\n",
    "\n",
    "def learning_infer(features, interval_ind):        \n",
    "    features = np.array(features).reshape(1, -1)\n",
    "    #loading trained models\n",
    "    if interval_ind == 30:\n",
    "        #standardizing\n",
    "        scaler = load('scaler_30.joblib')\n",
    "        features_transformed = scaler.transform(features)\n",
    "        clf_svc = load('trained_ev_detect_svm_30.joblib')\n",
    "        eclf = load('trained_ev_detect_ensemble_30.joblib')\n",
    "    elif interval_ind == 15:\n",
    "        scaler = load('scaler_15.joblib')\n",
    "        features_transformed = scaler.transform(features)\n",
    "        clf_svc = load('trained_ev_detect_svm_15.joblib')\n",
    "        eclf = load('trained_ev_detect_ensemble_15.joblib')\n",
    "    else:\n",
    "        return 0\n",
    "    ensemble_Y = eclf.predict(features_transformed)\n",
    "    svm_Y = clf_svc.predict(features_transformed)\n",
    "    #if they are not the same...\n",
    "    return ensemble_Y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #file reading\n",
    "    item = 'location_id=00000000-0000-000b-012c-135efde15403.csv'\n",
    "    #item = 'location_id=00000000-0000-000b-0264-6d7e5f610404.csv'\n",
    "    interval_ind = interval(item)\n",
    "    #print(interval_ind)\n",
    "    fid, output_feature, f = compressing_file_unsupervised(item, interval_ind)\n",
    "    print(fid)\n",
    "    bin_output = learning_infer(output_feature, interval_ind)\n",
    "    print(bin_output+f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
